---
title: "Using tidyverse tools with Pew Research Center survey data in R"
author: "Nick Hatley"
date: "`r format(Sys.Date())`"
output:
  blogdown::html_page
categories: ["R"]
slug: tidyverse-survey data
---

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri(here::here("resources", "center_wheel_logo.png")),
               alt = 'logo',
               style = 'position:absolute; top:0; right:0; padding:10px;')
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(message = FALSE)
options(knitr.table.format = "html") 
library(tidyverse)
library(haven)
Apr17 <- read_sav(here::here("data", "Apr17 public.sav"), user_na = TRUE) %>% as_factor()
```

## What is the "tidyverse"?
R is free and open-source software that anyone can contribute to. The most common kinds of contributions are collections of R code called [packages](https://cran.r-project.org/web/packages/). R packages are typically hosted on [The Comprehensive R Archive Network](https://cran.r-project.org/) (CRAN), but can also be found on other primarily open-source code repositories like [github](https://github.com/), [gitlab](https://about.gitlab.com/), or [bioconductor](https://www.bioconductor.org/). The ability for anyone to be able to build an R package and contribute to R's development is a key component of R's utility, but with so many packages (13,581 packages on CRAN as of 12/27/18), there are often many different ways to tackle the same challenge. This can lead to inconsistencies in R syntax that are often confusing and make it harder new users or those looking to develop their R skills to do so. 

Enter the [tidyverse](https://www.tidyverse.org/) -- a collection of R packages designed for data science that share a consistent design philosophy and produce code that can be more easily understood and thus shared among people. The tidyverse also has a [growing community](https://community.rstudio.com/c/tidyverse) of users, contributors, and developers that aim to help R users access and learn these free, open-source tools. The book *R for Data Science* by Hadley Wickham and Garrett Grolemund walks through many key components of how to use these tools to do data science in R and can be [accessed freely online](https://r4ds.had.co.nz/).  

Earlier this year, I wrote an [introductory](https://medium.com/pew-research-center-decoded/how-to-analyze-pew-research-center-survey-data-in-r-f326df360713) post on how to access and analyze Pew Research Center survey data using R.

There, I showed how to analyze Center data using the [survey](https://cran.r-project.org/web/packages/survey/index.html) package. The `survey` package is ideal if you are analyzing survey data and need variance estimates that account for the complex sample design (e.g. weighting or stratification). However, the first step in any data analysis is often to explore the data with simple but powerful summaries such as crosstabs and plots. This usually requires a lot of recoding variables, data cleaning and other data manipulation tasks. The `tidyverse` offers many straightforward tools to accomplish that. Here, I show how to use `tidyverse` tools to do explatory analyses of Pew Research Center survey data. (Of course, these tools can also be used with survey data from any source.)


## Packages 
For this post we will need the `tidyverse` and `haven` packages. The [haven package](https://haven.tidyverse.org/) is the tidyvere's solution for importing and exporting data from several different formats, including SPSS (the format in which most Center datasets are released), SAS, and Stata.    

The only functions from the haven package we will use here are `read_sav` and `as_factor`; 
all other functions referenced in this post come from packages like `dplyr`, `forcats`, or `ggplot2` that are loaded automatically when we load the [tidyverse](https://www.tidyverse.org/packages/) package. 

If you don't already have these packages, to run the code in this post, you will first need to install the packages. Even if you have already installed these packages, you can update them by running the below code: 
```{r install pkgs, echo=TRUE, eval=FALSE}
install.packages("tidyverse")
install.packages("haven") 
```

Then, load the packages in with the `library()` function:
```{r load pkgs, echo=TRUE, eval=FALSE}
library(tidyverse) #loads all "core" tidyverse packages like dplyr, tidyr, forcats, and ggplot2
library(haven) 
```

### Pipes 
A key element in the readability of tidyverse code is the pipe, a set of three characters that look like this: `%>%`. Placing a pipe tells R to take what's on its left and *pipe* it to its right. This allows you to chain multiple commands together without getting lost in a hailstorm of parentheses.  

For example, using the `starwars` dataset that comes attached to the `tidyverse` package, we can write two equivalent lines of code that filters the dataset to all characters for whom a height is known, arranges the dataset in order of decreasing height, and changes height from centimeters to inches.

The first, without the pipe:
```{r, echo=TRUE, eval=FALSE}  
mutate(arrange(filter(starwars, !is.na(height)), desc(height)), height = 0.393701 * height)
```
With the pipe, the code would be: 
```{r, echo=TRUE, eval=FALSE}  
starwars %>% 
  filter(!is.na(height)) %>%
  arrange(desc(height)) %>%
  mutate(height = 0.393701 * height)
```

As far as R is concerned, these two pieces of code do exactly the same thing, but using a pipeline makes it much easier for humans to read and understand exactly what’s going on and in what order. The code above can be easily read a series of steps: 1) take the starwars dataset, 2) filter it to cases where height is not missing, 3) then arrange the remaining cases by height in descending order, and finally 4) change the height variable to inches. Readability makes it easier to share your code with other people. Even if you’ll never share your code with anyone else, this approach will be easier for you to understand your own code if you ever need to revisit it weeks, months, or even years later when the details are no longer fresh in your memory. 

Beyond readability, pipes make your code easier to maintain. It’s easy to remove a step, insert a new step, or change the order when all of the commands are in a sequential pipeline rather than nested. 


##Loading the Data into R 
Datasets available for download can be accessed via the 'Datasets' tab on the Center website or via [this link](http://www.pewresearch.org/download-datasets/). For more information about the kind of data the Center releases and more details about how to access that data, see [this post](http://www.pewresearch.org/download-datasets/).   

For this post, we will use the [April 2017 Political Survey](http://www.people-press.org/dataset/april-2017-political-survey/) data. We will walk through how to use tidyverse tools to calculate weighted survey estimates (President Trump’s approval) by other variables in our dataset (education, race/ethnicity, and generation).

Almost all of the datasets available to download from the Center are stored as .sav (SPSS) files. SPSS files often contain variables with both numeric values and character labels (e.g. for party ID, 1 = Republican, 2 = Democrat). When loading the data, the user has to decide which of the values or value labels she wants to the dataset to contain in R.

The first step is to load the dataset into your R environment. We accomplish this with the `read_sav` function from haven. We set `user_na = TRUE` to ensure that responses such as "Don’t Know" or "Refused" aren’t automatically converted to missing values. We want the dataset to be read in with value labels (e.g. "Republican" instead of 1") and for categorical variables be stored as [factors](https://r4ds.had.co.nz/factors.html) so we then pipe (`%>%`) that into haven’s `as_factor` function.

```{r load data, echo=TRUE, eval=FALSE}
Apr17 <- read_sav("Apr17 public.sav", user_na = TRUE) %>% as_factor()
```

## Adding variables with `mutate`
The first question (`q1`) on the April 2017 survey asked whether or not respondents approved of President Donald Trump's performance as president so far. The following question (`q1a`) asked respondents how strongly they approved or disapproved. We want to use these two variables to create a new variable that combines `q1` and `q1a`. To do this we use the [mutate](https://dplyr.tidyverse.org/reference/mutate.html) function. `mutate` is a[ `dplyr`](https://dplyr.tidyverse.org/) function (that is loaded automatically when calling `library(tidyverse)`). It adds new variables to a data frame or modifies existing variables. We use the code below to create a variable called `trump_approval`.

It can be tricky to create new variables that are composites of two or more existing variables using only basic R functions, but it is made easier with the [case_when](https://dplyr.tidyverse.org/reference/case_when.html) function from `dplyr`. Inside the `mutate` function, we use `case_when` to assign our new categories. 

```{r make Trump approval, echo=TRUE}
Apr17 <- Apr17 %>% 
  mutate(trump_approval = case_when(
    q1 == "Approve" & q1a == "Very strongly" ~ "Strongly approve",
    q1 == "Approve" & q1a == "Not so strongly" ~ "Not strongly approve",
    q1 == "Disapprove" & q1a == "Very strongly" ~ "Strongly disapprove",
    q1 == "Disapprove" & q1a == "Not so strongly" ~ "Not strongly disapprove",
    q1 == "Don't know/Refused (VOL.)" | q1a == "Don't know/Refused (VOL.)" ~ "Refused"
                                  ) #this parentheses closes our call to case_when
                                    %>% #and then sends it to fct_relevel with %>% 
    fct_relevel("Strongly approve",
                "Not strongly approve",
                "Not strongly disapprove",
                "Strongly disapprove",                             
                "Refused"                
                ) #this parentheses closes our call to fct_relevel
  ) #this parentheses closes our call to mutate
```


We supply a series of formulas to the `case_when` function. Each formula is an if-then statement, where the left hand side (everything to the left of `~`) describes a logical condition, and the right-hand side provides the value to return if that condition is true. As a reminder, you can type `?case_when()` to access the help page for the case_when or other R functions.

The first line of the call to `case_when` is: `q1 == "Approve" & q1a == "Very strongly" ~ "Strongly approve"`. This can be read to mean that respondent said "Approve" to q1 and answered "Very Strongly" to q1a, code them as "Strongly approve" in our new `trump_approval` variable. We use five different clauses to make these new categories and add the `trump_approval` variable to our dataset. 

Then we pipe the variable we created with `case_when` to the [fct_relevel](https://forcats.tidyverse.org/reference/fct_relevel.html) function from the [forcats](https://forcats.tidyverse.org/) package. This coerces `trump_approval` to a factor variable and orders it from "Strongly approve" to "Refused."

We can run the `table` command on our new variable to verify that everything worked as intended.
```{r Trump approval table, echo=TRUE}
table(Apr17$trump_approval, Apr17$q1, useNA = "always")
```

In this example, we will look at how our new `trump_approval` variable breaks down according to a few different demographics: educational attainment (`educ2` in the Apr17 dataset), race/ethnicty (`racethn`), and generation (`gen5`).

But, `educ2` has 9 categories, so it is beneficial to collapse them to fewer categories with larger numbers of respondents. We can do this with the `forcats` function [fct_collapse](https://forcats.tidyverse.org/reference/fct_collapse.html). We use `mutate` again to make new variables. 

Let's first check the answer options of `educ2` to see the categories we need to collapse. Since we used `as_factor` when we read the dataset in, `educ2` is a factor variable. So, we can see the answer options by using the `levels` function.
```{r levels, echo=TRUE}
levels(Apr17$educ2)
```

Now that we know the levels, we can collapse them into fewer categories. The first argument to `fct_collapse` is the variable you want to collapse into fewer categories, in our case `educ2`. We assign new categories on the left, like "High School or Grad or Less", in the below example. We use the `c()` function to tell `fct_collapse` these categories belong in our new "High School or Grad or Less" category. Note that you need to use the exact names of these categories!

```{r collapse educ and relig, echo=TRUE}
Apr17 = Apr17 %>% 
  mutate(educ_cat = fct_collapse(educ2,
          "High School Grad or Less" = c(
                    "Less than high school (Grades 1-8 or no formal schooling)",
                    "High school incomplete (Grades 9-11 or Grade 12 with NO diploma)",                         
                    "High school graduate (Grade 12 with diploma or GED certificate)"),
          "Some college" = c(
                    "Some college, no degree (includes some community college)",                                
                    "Two year associate degree from a college or university"),
           "College grad+" = c(
                    "Four year college or university degree/Bachelor's degree (e.g., BS, BA, AB)",              
                    "Some postgraduate or professional schooling, no postgraduate degree",                      
                    "Postgraduate or professional degree, including master's, doctorate, medical or law degree")
                                ) #this parentheses closes our call to fct_collapse
      ) #this parentheses closes our call to mutate
```


Again, we can do a table on our new variable to check everything worked as intended.
```{r educ_cat table, echo=TRUE}
table(Apr17$educ_cat, useNA = "always")
```

## Getting weighted estimates with group_by and summarise
Now that we have created and recoded the variables we wish to estimate, we can use [group_by](https://dplyr.tidyverse.org/reference/group_by.html) and [summarise](https://dplyr.tidyverse.org/reference/summarise.html) to produce some weighted summaries of the data. We use `group_by` to tell R to group the Apr17 dataset by these variables, and then `summarise` to create summary statistics among those groups.

To make sure our estimates are representative of the population we need to use the survey weights (variable named `weight`) included in our (and almost every Center) dataset. Inside our `summarise` call we first get the weighted total of the groups we are intereseted in. We do this by taking a sum of the `weight` variable.

So, the first step in code is: 
```{r weighted_total, echo=TRUE}
trump_estimates_educ = Apr17 %>% 
  group_by(educ_cat, trump_approval) %>% 
  summarise(weighted_n = sum(weight)) 
```

If we look at the `trump_estimates_educ` object that we just created with `group_by` and `summarise`, we can see we have a column for each of the variables we passed to `group_by` (`educ_cat` and `trump_approval`) and a column we created called `weighted_n`. The `weighted_n` column is the weighted sum of each category in the `trump_approval` variable we just created by education.

```{r weighted_total educ show, echo=TRUE}
trump_estimates_educ
```

But, since we want to show proportions we have another step to do. After calculating the `weighted_n` of each education category by Trump approval, we now need to know the weighted group size of the education categories overall. We do this by grouping by **only** `educ_cat`. Then, we use mutate to add a column of the sum of each educaation categories weighted total size. This leaves us with a weighted total of how many people are in that education category and a weighted total for how many people in that education category "Strongly Approve" through "Strongly Disapprove" in `trump_approval`. To calculate proportions all we have to do is divide the weighted total group size `weighted_group_size` by the `weighted_n` we calculated earlier.

```{r weighted_percentages, echo=TRUE}
trump_estimates_educ = Apr17 %>% 
  #group by education and trump approval
  group_by(educ_cat, trump_approval) %>% 
  #calculate the total number of people in each answer and education category using survey weights (weight)
  summarise(weighted_n = sum(weight)) %>% 
  #group by education to calculate education category size
  group_by(educ_cat) %>%
  #add columns for total group size and the proportion
  mutate(weighted_group_size = sum(weighted_n),
         weighted_estimate = weighted_n/weighted_group_size)

trump_estimates_educ
```

But, `educ2` is not the only variable we are interested in for this analysis. We know we need to add the `racethn` and `gen5` variables at some step. With some reshaping of the data using the `gather` function, we can use the same steps we used above with `educ_cat` to get all of our subgroup summaries at once. 

##Rearranging data with `gather()`
The first step to make the process a bit simpler is taking the `Apr17` dataset and selecting down to only the columns we need for our analysis using the [select](https://dplyr.tidyverse.org/reference/select.html) function. The `select` function is like a swiss-army knife of keeping, rearranging, or dropping (using `-`) columns. There are also a number of [helper functions](https://www.rdocumentation.org/packages/tidyselect/versions/0.2.5/topics/select_helpers) like `starts_with()`, `ends_with()`, `matches()` that make it easy to select a number of columns with a certain pattern instead of naming each column explicitly.
```{r Apr17 select, echo=TRUE}
Apr17 = Apr17 %>% 
  select(trump_approval, weight, educ_cat, racethn, gen5)
```

This line of code with the `select()` function will change the `Apr17` object to contain only the columns we need for our analysis (`trump_approval`, the survey weight `weight`, or one of the subgroup variables we are interested in).

```{r Apr17 select head, echo=TRUE}
head(Apr17)
```

Now that we have selected `Apr17` down to the columns we want for our analysis, the next step is to rearrange the data in a way that will be easy to calculate the weighted summary statistics by each subgroup (`educ_cat`, `racethn`, and `gen5`) we are interested in. We do this rearranging with the [gather](https://tidyr.tidyverse.org/reference/gather.html) function. The `gather` function is an extremely useful function when working with data. 

It reshapes data from "wide" to "long" format to contain one new column that serves as a "key" and corresponding column for a "value". The goal here is to combine our three subgroup columns into a pair of columns; one will be a "key" column (`subgroup_variable`) that tells us where the subgroup value comes from (one of `educ_cat`, `racethn`, or `gen5`) and a "value" column called `subgroup` that reflects the corresponding value of our subgroup columns.

```{r Apr17 gather, echo=TRUE}
Apr17_long = Apr17 %>% 
  #gather educ_cat, racethn, gen5 into two columns: 
  ##a key  called "subgroup variable" (educ_cat, racethn, gen5)
  ##and a value called "subgroup" ("High School Grad or Less", "Black, non-Hispanic", "Millenial (1981-") 
  gather(key = subgroup_variable, value = subgroup, educ_cat:gen5) 
```

The `educ_cat:gen5` section of the call to `gather()` can be read as "gather all columns from `educ_cat` through `gen5`". After the `gather()` step, this leaves us with a "long" dataset of now 4,503 rows.


```{r fig.cap='*TKTKTK insert graphic Bill is working on* similar to https://github.com/apreshill/teachthat/blob/master/gather/gather-gif/gather-01.png', fig.align='center', out.width = "80%", out.height= "80%"}
knitr::include_graphics(here::here("resources", "19.01.07_decodedTidyverse640px.png"))
```


```{r Apr17_long show, echo=TRUE}
Apr17_long
```

This leaves us with a column that indicates a respondent's answer to `trump_approval`, their survey weight (`weight`), an indidator for the subgroup variable (`subgroup_variable`), and their subgroup value (`subgroup`). 

We can check to make sure all of our subgroups are in the `Apr17_long` with the `table()` function. 

```{r Apr17 tables, echo=TRUE}
table(Apr17_long$subgroup_variable, useNA = "always")
table(Apr17_long$subgroup, useNA = "always")
```


If you want to read more about `gather`, I would recommend starting with [this tweet](https://twitter.com/WeAreRLadies/status/1059520693857996800) by [Alison Hill](https://twitter.com/apreshill) from the [WeAreRLadies](https://twitter.com/WeAreRLadies) twitter account. You can also go the [Tidy Data](https://r4ds.had.co.nz/tidy-data.html#gathering) chapter in the previously mentioned *R for Data Science* book.


Now that we have our data arrange in this format, we can follow the same steps we used to get weighted summaries for `educ_cat` on all three subgroup variables at once. Instead of passing `educ_cat` to the `group_by()` function we pass our new `subgroup_variable` and `subgroup` columns.

```{r weighted_percentages all subgroups, echo=TRUE}
trump_estimates = Apr17_long %>% 
  #group by subgroup_variable, subgroup, and trump approval
  group_by(subgroup_variable, subgroup, trump_approval) %>% 
  #calculate the total number of people in each answer and education category using survey weights (weight)
  summarise(weighted_n = sum(weight)) %>% 
  #group by subgroup only to calculate subgroup category size
  group_by(subgroup) %>%
  #add columns for total group size and the proportion
  mutate(weighted_group_size = sum(weighted_n),
         weighted_estimate = weighted_n/weighted_group_size)

```

Since we are only really interested in the proportions we remove the `weighted_total` and `weighted_group_size` columns using the `select()` function. Including the `-` before a column name tells `select()` to drop that column.

```{r weighted_percentages select, echo=TRUE}
trump_estimates = trump_estimates %>% 
  select(-weighted_n, -weighted_group_size) 
```

Here's what we end up with. 
```{r weighted_percentages show, echo=TRUE}
trump_estimates
```

Arranging the data this way makes it easy to create plots or tables of the variables of interest.
Below is an example of a simple plot of the estimates we created in this post. We use the [filter()](https://dplyr.tidyverse.org/reference/filter.html) function to remove the "Refused" category in the trump_approval variable and in any of our subgroup variables.

```{r weighted_percentages plot, echo=TRUE, fig.width=8.5, fig.height=5, eval=TRUE}
trump_estimates %>% 
  filter(trump_approval != "Refused") %>% 
  filter(!(subgroup %in% c("Don't know/Refused (VOL.)", "DK/Ref"))) %>% 
  ggplot(
    aes(
      x = weighted_estimate,
      y = subgroup
    )
  ) +
  geom_point() + 
  scale_x_continuous(limits = c(0, 1),
                     breaks = seq(0,1, by = .2),
                     labels = scales::percent(seq(0,1, by = .2), accuracy = 1)
  ) +
  facet_grid(cols = vars(trump_approval),
             rows = vars(subgroup_variable), 
             scales = "free_y",
             space = "free"
             ) +
  theme_bw() +
  theme(axis.title.y = element_blank())
```

Here is all the necessary code used in the post 

```{r all code, echo=TRUE, eval=FALSE}
#code from start to finish 

##install or update packages if neccessary
#install.packages("tidyverse")
#install.packages("haven") 

##load packages in 
library(tidyverse) #loads all "core" tidyverse packages like dplyr, tidyr, forcats, and ggplot2
library(haven) 

##read dataset in with value labels (as_factor)
Apr17 <- read_sav("Apr17 public.sav", user_na = TRUE) %>% as_factor()

##create trump_approval variable and relevel it 
Apr17 <- Apr17 %>% 
  mutate(trump_approval = case_when(
    q1 == "Approve" & q1a == "Very strongly" ~ "Strongly approve",
    q1 == "Approve" & q1a == "Not so strongly" ~ "Not strongly approve",
    q1 == "Disapprove" & q1a == "Very strongly" ~ "Strongly disapprove",
    q1 == "Disapprove" & q1a == "Not so strongly" ~ "Not strongly disapprove",
    q1 == "Don't know/Refused (VOL.)" | q1a == "Don't know/Refused (VOL.)" ~ "Refused"
                                  ) #this parentheses closes our call to case_when
  %>% #and then sends it to fct_relevel with %>% 
    fct_relevel("Strongly approve",
                "Not strongly approve",
                "Not strongly disapprove",
                "Strongly disapprove",                             
                "Refused"                
    ) #this parentheses closes our call to fct_relevel
  ) #this parentheses closes our call to mutate

## collapse education variable into 3 categories
Apr17 = Apr17 %>% 
  mutate(educ_cat = fct_collapse(educ2,
          "High School Grad or Less" = c(
                    "Less than high school (Grades 1-8 or no formal schooling)",
                    "High school incomplete (Grades 9-11 or Grade 12 with NO diploma)",                         
                    "High school graduate (Grade 12 with diploma or GED certificate)"),
          "Some college" = c(
                    "Some college, no degree (includes some community college)",                                
                    "Two year associate degree from a college or university"),
           "College grad+" = c(
                    "Four year college or university degree/Bachelor's degree (e.g., BS, BA, AB)",              
                    "Some postgraduate or professional schooling, no postgraduate degree",                      
                    "Postgraduate or professional degree, including master's, doctorate, medical or law degree")
                                ) #this parentheses closes our call to fct_collapse
      ) #this parentheses closes our call to mutate

##get trump_approval by education 
trump_estimates_educ = Apr17 %>% 
  #group by education and trump approval
  group_by(educ_cat, trump_approval) %>% 
  #calculate the total number of people in each answer and education category using survey weights (weight)
  summarise(weighted_n = sum(weight)) %>% 
  #group by education to calculate education category size
  group_by(educ_cat) %>%
  #add columns for total group size and the proportion
  mutate(weighted_group_size = sum(weighted_n),
         weighted_estimate = weighted_n/weighted_group_size)


##select only columns interested in for this analysis
Apr17 = Apr17 %>% 
  select(trump_approval, weight, educ_cat, racethn, gen5)

##create Apr_17 long with gather
Apr17_long = Apr17 %>% 
  #gather educ_cat, racethn, gen5 into two columns: 
  ##a key  called "subgroup variable" (educ_cat, racethn, gen5)
  ##and a value called "subgroup" ("High School Grad or Less", "Black, non-Hispanic", "Millenial (1981-") 
  gather(key = subgroup_variable, value = subgroup, educ_cat:gen5) 


##get weighted estimates for every subgroup 
trump_estimates = Apr17_long %>% 
  #group by subgroup_variable, subgroup, and trump approval
  group_by(subgroup_variable, subgroup, trump_approval) %>% 
  #calculate the total number of people in each answer and education category using survey weights (weight)
  summarise(weighted_n = sum(weight)) %>% 
  #group by subgroup only to calculate subgroup category size
  group_by(subgroup) %>%
  #add columns for total group size and the proportion
  mutate(weighted_group_size = sum(weighted_n),
         weighted_estimate = weighted_n/weighted_group_size) %>% 
  #only want proportions so select out total categories
  select(-weighted_n, -weighted_group_size) 

##create plot
trump_estimates %>% 
  filter(trump_approval != "Refused") %>% 
  filter(!(subgroup %in% c("Don't know/Refused (VOL.)", "DK/Ref"))) %>% 
  ggplot(
    aes(
      x = weighted_estimate,
      y = subgroup
    )
  ) +
  geom_point() + 
  scale_x_continuous(limits = c(0, 1),
                     breaks = seq(0,1, by = .2),
                     labels = scales::percent(seq(0,1, by = .2), accuracy = 1)
  ) +
  facet_grid(cols = vars(trump_approval),
             rows = vars(subgroup_variable), 
             scales = "free_y",
             space = "free"
  ) +
  theme_bw() +
  theme(axis.title.y = element_blank())

```

